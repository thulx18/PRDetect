{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta-base/ were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载英语模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=False)\n",
    "model = RobertaModel.from_pretrained(\"./roberta-base/\")\n",
    "tokenizer = RobertaTokenizer(\"./roberta-base/vocab.json\", \"./roberta-base/merges.txt\", use_fast=False)\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# texts = list()\n",
    "# with open(\"original_text/hc3human.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f.readlines():\n",
    "#         texts.append(line)\n",
    "# with open(\"original_text/hc3chatgpt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f.readlines():\n",
    "#         texts.append(line)\n",
    "# print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(42)\n",
    "\n",
    "# random.shuffle(texts)\n",
    "\n",
    "# with open(\"original_text/hc3_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(0, 8000):\n",
    "#         f.write(texts[i])\n",
    "# with open(\"original_text/hc3_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(8000, 9000):\n",
    "#         f.write(texts[i])\n",
    "# with open(\"original_text/hc3_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(9000, 10000):\n",
    "#         f.write(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "texts = list()\n",
    "with open(\"original_text/hc3human.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        text = json.loads(line.strip())['text']\n",
    "        texts.append(text)\n",
    "with open(\"original_text/hc3chatgpt.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        text = json.loads(line.strip())['text']\n",
    "        texts.append(text)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [27:04<00:00,  6.16it/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = list()\n",
    "all_token_embeddings = list()\n",
    "all_edge_index = list()\n",
    "all_sparse_adj_matrix = list()\n",
    "for text in tqdm(texts):\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        tokenized_sentence = [token.text for token in doc]\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "        # print(tokenized_sentence)\n",
    "        \n",
    "        max_length = 512\n",
    "        chunks = [tokenized_sentence[i:i+max_length] for i in range(0, len(tokenized_sentence), max_length)]\n",
    "        chunk_outputs = []\n",
    "        for chunk in chunks:\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(chunk)\n",
    "            input_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids)\n",
    "\n",
    "            last_hidden_states = output.last_hidden_state\n",
    "            token_embeddings = last_hidden_states[0]\n",
    "            chunk_outputs.append(token_embeddings)\n",
    "        token_embeddings = torch.cat(chunk_outputs, dim=0)\n",
    "        all_token_embeddings.append(token_embeddings)\n",
    "        # print(len(tokenized_sentence))\n",
    "        # print(token_embeddings.shape)\n",
    "        node_relations = list()\n",
    "        for i,word in enumerate(doc):        \n",
    "            node_relations.append([word.i,word.head.i])\n",
    "            # 加上自环\n",
    "            if word.i != word.head.i:\n",
    "                node_relations.append([word.i,word.i])\n",
    "        edge0 = list()\n",
    "        edge1 = list()\n",
    "        for edge in node_relations:\n",
    "            edge0.append(edge[0])\n",
    "            edge1.append(edge[1])\n",
    "        edge_index = torch.tensor([edge0, edge1], dtype=torch.long)\n",
    "        all_edge_index.append(edge_index)\n",
    "        # sparse_adj_matrix = csr_matrix((np.ones(len(edge0)),(np.array(edge0), np.array(edge1))),shape=(len(tokenized_sentence),len(tokenized_sentence)))\n",
    "        # dependency_matrix = sparse_adj_matrix\n",
    "        # print(sparse_adj_matrix)\n",
    "        # all_sparse_adj_matrix.append(sparse_adj_matrix)\n",
    "    except Exception as e:\n",
    "        print(text)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.cat((torch.zeros(5000), torch.ones(5000)), dim = 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"graph_data/hc3_all_token_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_token_embeddings, f)\n",
    "with open(\"graph_data/hc3_all_edge_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_edge_index, f)\n",
    "with open(\"graph_data/hc3_y.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 构建 GCN 模型\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,  input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        self.fc = nn.Linear(output_dim, 1) \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch.mean(x, dim=0, keepdim=True)  \n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# with open(\"graph_data/hc3_all_token_embeddings.pkl\", \"rb\") as f:\n",
    "#     all_token_embeddings = pickle.load(f)\n",
    "# with open(\"graph_data/hc3_all_edge_index.pkl\", \"rb\") as f:\n",
    "#     all_edge_index = pickle.load(f)\n",
    "# with open(\"graph_data/hc3_y.pkl\", \"rb\") as f:\n",
    "#     y = pickle.load(f)\n",
    "# print(len(all_token_embeddings), len(all_edge_index), len(y))\n",
    "\n",
    "combined_list = list(zip(all_token_embeddings, all_edge_index, y))\n",
    "random.shuffle(combined_list)\n",
    "all_token_embeddings, all_edge_index, y = zip(*combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "input_dim = 768  # 输入维度\n",
    "hidden_dim = 256  # 隐藏层维度\n",
    "output_dim = 2  # 输出类别数\n",
    "train_len = int(len(all_token_embeddings)*0.8)\n",
    "val_len = len(all_token_embeddings) - train_len\n",
    "gcnmodel = GCN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(gcnmodel.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, Training: 100%|██████████| 8000/8000 [00:31<00:00, 252.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 0.34135247974603705, train_acc: 0.850125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 623.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, val_loss: 0.17168363584279023, val_acc: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, Training: 100%|██████████| 8000/8000 [00:31<00:00, 252.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, train_loss: 0.16874683047063344, train_acc: 0.944875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 614.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, val_loss: 0.1183810183384536, val_acc: 0.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, Training: 100%|██████████| 8000/8000 [00:32<00:00, 249.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, train_loss: 0.12373776972758295, train_acc: 0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 623.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, val_loss: 0.09451926180805875, val_acc: 0.9685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, Training: 100%|██████████| 8000/8000 [00:31<00:00, 253.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, train_loss: 0.09552198397244936, train_acc: 0.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 631.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, val_loss: 0.08200260208252942, val_acc: 0.9725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, Training: 100%|██████████| 8000/8000 [00:31<00:00, 253.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, train_loss: 0.07692710811232956, train_acc: 0.977375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 624.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, val_loss: 0.08224168126330697, val_acc: 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, Training: 100%|██████████| 8000/8000 [00:31<00:00, 253.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, train_loss: 0.06167388150758772, train_acc: 0.983375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 622.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, val_loss: 0.08533512484207548, val_acc: 0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7, Training: 100%|██████████| 8000/8000 [00:31<00:00, 252.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.04846011193181407, train_acc: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 623.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, val_loss: 0.09491106975913294, val_acc: 0.9725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8, Training: 100%|██████████| 8000/8000 [00:32<00:00, 249.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, train_loss: 0.04037994268652748, train_acc: 0.98975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 621.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, val_loss: 0.09670232109449446, val_acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, Training: 100%|██████████| 8000/8000 [00:32<00:00, 242.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, train_loss: 0.03317827793079549, train_acc: 0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 619.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, val_loss: 0.09012448973740483, val_acc: 0.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10, Training: 100%|██████████| 8000/8000 [00:33<00:00, 241.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, train_loss: 0.03157115008249196, train_acc: 0.99275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 622.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, val_loss: 0.1191908633668014, val_acc: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, train_loss: 0.03253350948001371, train_acc: 0.992125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 618.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, val_loss: 0.111037132947899, val_acc: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, train_loss: 0.025883183552748032, train_acc: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 617.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, val_loss: 0.1696719238373939, val_acc: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13, Training: 100%|██████████| 8000/8000 [00:33<00:00, 241.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, train_loss: 0.022459803556593834, train_acc: 0.995875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 618.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, val_loss: 0.10392842729426462, val_acc: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14, Training: 100%|██████████| 8000/8000 [00:33<00:00, 241.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, train_loss: 0.023162085708960715, train_acc: 0.995625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 622.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, val_loss: 0.16084086552554713, val_acc: 0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, train_loss: 0.02067339141732088, train_acc: 0.995625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 617.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, val_loss: 0.15628841045508812, val_acc: 0.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, train_loss: 0.02216562623076597, train_acc: 0.99525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 624.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, val_loss: 0.1103282908285541, val_acc: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, train_loss: 0.019099550314535363, train_acc: 0.996125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 612.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, val_loss: 0.1286017796114064, val_acc: 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, train_loss: 0.017194813125385316, train_acc: 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 619.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, val_loss: 0.11227779213108033, val_acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19, Training: 100%|██████████| 8000/8000 [00:33<00:00, 239.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, train_loss: 0.01681230262956831, train_acc: 0.99725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 619.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, val_loss: 0.2680402272533756, val_acc: 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20, Training: 100%|██████████| 8000/8000 [00:34<00:00, 234.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, train_loss: 0.017770611136053473, train_acc: 0.996625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20, Validation: 100%|██████████| 2000/2000 [00:03<00:00, 623.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, val_loss: 0.27335069010438745, val_acc: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for epoch in range(epochs):\n",
    "    # 训练集\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    for i in tqdm(range(train_len),  f\"epoch: {epoch+1}, Training\"):\n",
    "        data = Data(x=all_token_embeddings[i], edge_index=all_edge_index[i], y=y[i])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gcnmodel(data)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, data.y.float().view(-1, 1))\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        predictions = (outputs >= 0.5).long()  \n",
    "        correct_predictions += (predictions == data.y.view(-1, 1)).sum().item()\n",
    "    epoch_loss /= train_len\n",
    "    epoch_acc = correct_predictions / train_len\n",
    "    print(f\"epoch: {epoch+1}, train_loss: {epoch_loss}, train_acc: {epoch_acc}\")\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    \n",
    "    # 验证集\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    for i in tqdm(range(train_len, len(all_token_embeddings)),  f\"epoch: {epoch+1}, Validation\"):\n",
    "        data = Data(x=all_token_embeddings[i], edge_index=all_edge_index[i], y=y[i])\n",
    "        outputs = gcnmodel(data)\n",
    "        loss = criterion(outputs, data.y.float().view(-1, 1))\n",
    "        epoch_loss += loss.item()\n",
    "        predictions = (outputs >= 0.5).long()\n",
    "        correct_predictions += (predictions == data.y.view(-1, 1)).sum().item()\n",
    "    epoch_loss /= val_len\n",
    "    epoch_acc = correct_predictions / val_len\n",
    "    print(f\"epoch: {epoch+1}, val_loss: {epoch_loss}, val_acc: {epoch_acc}\")\n",
    "    val_loss.append(epoch_loss)\n",
    "    val_acc.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gcnmodel.state_dict(), './model/gcn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9745"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e169789a8a24bd293e0e567b1a4f7ab47d7e5c50178991eeca607254e2c130d4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
